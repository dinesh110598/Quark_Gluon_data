{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py as h5\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric import nn as gnn\n",
    "\n",
    "from graph_vae import GraphVAE\n",
    "from train import train_loop\n",
    "from data_load import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = GraphVAE(3, 32, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "nonzero(input, *, out=None, as_tuple=False) -> LongTensor or tuple of LongTensors\n",
      "\n",
      ".. note::\n",
      "    :func:`torch.nonzero(..., as_tuple=False) <torch.nonzero>` (default) returns a\n",
      "    2-D tensor where each row is the index for a nonzero value.\n",
      "\n",
      "    :func:`torch.nonzero(..., as_tuple=True) <torch.nonzero>` returns a tuple of 1-D\n",
      "    index tensors, allowing for advanced indexing, so ``x[x.nonzero(as_tuple=True)]``\n",
      "    gives all nonzero values of tensor ``x``. Of the returned tuple, each index tensor\n",
      "    contains nonzero indices for a certain dimension.\n",
      "\n",
      "    See below for more details on the two behaviors.\n",
      "\n",
      "    When :attr:`input` is on CUDA, :func:`torch.nonzero() <torch.nonzero>` causes\n",
      "    host-device synchronization.\n",
      "\n",
      "**When** :attr:`as_tuple` **is** ``False`` **(default)**:\n",
      "\n",
      "Returns a tensor containing the indices of all non-zero elements of\n",
      ":attr:`input`.  Each row in the result contains the indices of a non-zero\n",
      "element in :attr:`input`. The result is sorted lexicographically, with\n",
      "the last index changing the fastest (C-style).\n",
      "\n",
      "If :attr:`input` has :math:`n` dimensions, then the resulting indices tensor\n",
      ":attr:`out` is of size :math:`(z \\times n)`, where :math:`z` is the total number of\n",
      "non-zero elements in the :attr:`input` tensor.\n",
      "\n",
      "**When** :attr:`as_tuple` **is** ``True``:\n",
      "\n",
      "Returns a tuple of 1-D tensors, one for each dimension in :attr:`input`,\n",
      "each containing the indices (in that dimension) of all non-zero elements of\n",
      ":attr:`input` .\n",
      "\n",
      "If :attr:`input` has :math:`n` dimensions, then the resulting tuple contains :math:`n`\n",
      "tensors of size :math:`z`, where :math:`z` is the total number of\n",
      "non-zero elements in the :attr:`input` tensor.\n",
      "\n",
      "As a special case, when :attr:`input` has zero dimensions and a nonzero scalar\n",
      "value, it is treated as a one-dimensional tensor with one element.\n",
      "\n",
      "Args:\n",
      "    input (Tensor): the input tensor.\n",
      "\n",
      "Keyword args:\n",
      "    out (LongTensor, optional): the output tensor containing indices\n",
      "\n",
      "Returns:\n",
      "    LongTensor or tuple of LongTensor: If :attr:`as_tuple` is ``False``, the output\n",
      "    tensor containing indices. If :attr:`as_tuple` is ``True``, one 1-D tensor for\n",
      "    each dimension, containing the indices of each nonzero element along that\n",
      "    dimension.\n",
      "\n",
      "Example::\n",
      "\n",
      "    >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))\n",
      "    tensor([[ 0],\n",
      "            [ 1],\n",
      "            [ 2],\n",
      "            [ 4]])\n",
      "    >>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
      "    ...                             [0.0, 0.4, 0.0, 0.0],\n",
      "    ...                             [0.0, 0.0, 1.2, 0.0],\n",
      "    ...                             [0.0, 0.0, 0.0,-0.4]]))\n",
      "    tensor([[ 0,  0],\n",
      "            [ 1,  1],\n",
      "            [ 2,  2],\n",
      "            [ 3,  3]])\n",
      "    >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)\n",
      "    (tensor([0, 1, 2, 4]),)\n",
      "    >>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
      "    ...                             [0.0, 0.4, 0.0, 0.0],\n",
      "    ...                             [0.0, 0.0, 1.2, 0.0],\n",
      "    ...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)\n",
      "    (tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))\n",
      "    >>> torch.nonzero(torch.tensor(5), as_tuple=True)\n",
      "    (tensor([0]),)\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "?torch.nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dinesh/Documents/Projects/Quark_Gluon_data/explore.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dinesh/Documents/Projects/Quark_Gluon_data/explore.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_loop(net, \u001b[39m2\u001b[39m, \u001b[39m32\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Projects/Quark_Gluon_data/train.py:31\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(net, epochs, batch_size, lr)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mfor\u001b[39;00m (x, m0, pt, y) \u001b[39min\u001b[39;00m data_loader:\n\u001b[1;32m     30\u001b[0m     opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 31\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(net, x)\n\u001b[1;32m     32\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     34\u001b[0m     opt\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/Projects/Quark_Gluon_data/train.py:17\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(net, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_fn\u001b[39m(net, x):\n\u001b[1;32m     15\u001b[0m     \u001b[39m# What terms to include ?\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[39m# x.shape = (batch, 125, 125, 3)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     X, A \u001b[39m=\u001b[39m preprocess(x)\n\u001b[1;32m     18\u001b[0m     Y, A2, mu, logvar, L1, L2 \u001b[39m=\u001b[39m net(X, A)\n\u001b[1;32m     19\u001b[0m     mse \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mMSELoss()\n",
      "File \u001b[0;32m~/Documents/Projects/Quark_Gluon_data/data_load.py:68\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess\u001b[39m(x):\n\u001b[0;32m---> 68\u001b[0m     graphs \u001b[39m=\u001b[39m graph_list(x)\n\u001b[1;32m     69\u001b[0m     counts \u001b[39m=\u001b[39m node_counter(graphs)\n\u001b[1;32m     70\u001b[0m     lengs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mLongTensor(np\u001b[39m.\u001b[39mhstack(assigner(counts)))\n",
      "File \u001b[0;32m~/Documents/Projects/Quark_Gluon_data/data_load.py:20\u001b[0m, in \u001b[0;36mgraph_list\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m     19\u001b[0m     ecal \u001b[39m=\u001b[39m X[i, :, :, \u001b[39m1\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m     xhit, yhit \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnonzero(ecal, as_tuple\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     21\u001b[0m     pos \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack((xhit\u001b[39m.\u001b[39mfloat(), yhit\u001b[39m.\u001b[39mfloat()), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m     E \u001b[39m=\u001b[39m ecal[xhit, yhit]\u001b[39m*\u001b[39m\u001b[39m50\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "train_loop(net, 2, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = h5.File(\"quark-gluon_data-set_n139306.hdf5\")\n",
    "x, m0, pt, y = (torch.from_numpy(f['X_jets'][:5000]), torch.from_numpy(f['m0'][:5000]), \n",
    "                torch.from_numpy(f['pt'][:5000]), torch.from_numpy(f['y'][:5000]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['y'][[4, 6, 7, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5000, 125, 125, 3]), torch.Size([5000]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, m0.shape\n",
    "# Later: different batch size and/or load full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
       "       1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = graph_list(x)\n",
    "counts = node_counter(graphs)\n",
    "lengs = torch.LongTensor(np.hstack(assigner(counts)))\n",
    "\n",
    "compress = torch_geometric.data.Batch.from_data_list(graphs)\n",
    "G = compress.x.clone() # All nodes of all graphs cat together\n",
    "E = compress.edge_index.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_batch, to_dense_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, mask = to_dense_batch(G, lengs, fill_value=0, max_num_nodes=1000)\n",
    "A = to_dense_adj(E, lengs, max_num_nodes=1000) # (batch, 1000, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1000, 3]), torch.Size([2, 1000, 1000]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X\n",
    "A = A\n",
    "X.shape, A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphVAE(\n",
       "  (sage): ModuleList(\n",
       "    (0): DenseSAGEConv(3, 16)\n",
       "    (1): DenseSAGEConv(16, 16)\n",
       "    (2): DenseSAGEConv(16, 3)\n",
       "  )\n",
       "  (drop): ModuleList(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Dropout(p=0.4, inplace=False)\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (batch_norm): ModuleList(\n",
       "    (0): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (pool): ModuleList(\n",
       "    (0): MinCut_Pool(\n",
       "      (linear): Linear(in_features=16, out_features=500, bias=True)\n",
       "    )\n",
       "    (1): MinCut_Pool(\n",
       "      (linear): Linear(in_features=16, out_features=250, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (tr_mu): Linear(in_features=3, out_features=16, bias=True)\n",
       "  (tr_var): Linear(in_features=3, out_features=16, bias=True)\n",
       "  (tr_rev): Linear(in_features=16, out_features=3, bias=True)\n",
       "  (revsage): ModuleList(\n",
       "    (0): DenseSAGEConv(16, 3)\n",
       "    (1): DenseSAGEConv(16, 16)\n",
       "    (2): DenseSAGEConv(3, 16)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = GraphVAE(3, 16, 3)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z, A, mu, logvar, L1, L2 = net(X, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1000, 3]), torch.Size([2, 1000, 1000]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.shape, A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
